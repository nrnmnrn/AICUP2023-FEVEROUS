# -*- coding: utf-8 -*-
"""AICUP_trainmodel

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TvA6sb2R4fhKeIefaVsr0p-Re9cEwJyr
"""

#pip install transformers datasets accelerate

#!nvidia-smi

"""# 抓dataset"""

from pprint import pprint

"""from datasets import load_dataset

dataset = load_dataset("glue", "mrpc")

## dataset裡分成三個 train(訓練用) validation(每次訓縣完測試訓練情形用) test(最終算分用)

print(dataset)
pprint(dataset['train'][1])

train_data=dataset['train']
test_data=dataset['test']
eval_data=dataset['validation']

import json
from pprint import pprint

pprint(train_data[0]['sentence1'])

## 把每一筆資料集的每一項拆開
"""

import csv
def read_data(dataset):
    with open(dataset,newline='')as csvfile:
        rows = csv.reader(csvfile)
        count=0
        sentence1=[]
        sentence2=[]
        label=[]
        for row in rows:
            if count==0:
                count+=1
                continue
            sentence1.append(row[0])
            sentence2.append(row[1])
            label.append(row[2])
            count+=1
    #open file
    
    
    return sentence1,sentence2,label

train_sen1,train_sen2,trainlabel=read_data("./Training Dataset_v2/訓練資料集/sentence_retrieval_train.csv")
print(len(train_sen1))
print(train_sen1[0])

test_sen1,test_sen2,testlabel=read_data("./Training Dataset_v2/訓練資料集/sentence_retrieval_test.csv")
print(len(test_sen1))
print(test_sen1[0])

eval_sen1,eval_sen2,evallabel=read_data('./Training Dataset_v2/訓練資料集/sentence_retrieval_val.csv')
#evalabel=eval_data['label']
print(len(eval_sen1))
print(eval_sen1[0])



"""# 選擇和使用tokenizer，具體要使用哪個tokenizer可以去huggingface官網找"""

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

tokenizer

train_encodings = tokenizer(train_sen1, train_sen2, truncation=True, padding=True)
print(train_encodings)
eval_encodings = tokenizer(eval_sen1, eval_sen2, truncation=True, padding=True)

train_encodings.keys()

"""print(tokenizer.decode(train_encodings['input_ids'][0]))
print(train_encodings['input_ids'][0])
print("token_type_ids\n", train_encodings['token_type_ids'][0])
print("attention_mask\n", train_encodings['attention_mask'][0])

print(len(train_encodings['input_ids'][0]))
print(len(train_encodings['token_type_ids'][0]))

## 加入label(答案)
"""

def add_targets(encodings,label):
    encodings.update({'label':label})
add_targets(train_encodings,trainlabel)
add_targets(eval_encodings,evallabel)

print(train_encodings.keys())
print(tokenizer.decode(train_encodings['label']))

"""# 定義dataset 並轉換成tensor格式

"""

from torch.utils import data
import torch

class Dataset(torch.utils.data.Dataset):
  def __init__(self, encodings):
    self.encodings = encodings

  def __getitem__(self, idx):
    return {key: torch.tensor(eval[idx]) for key, eval in self.encodings.items()}

  def __len__(self):
    return len(self.encodings.input_ids)

train_dataset = Dataset(train_encodings)
eval_dataset = Dataset(eval_encodings)

train_dataset[0]

"""# 載入模型架構"""

from transformers import BertConfig, BertForSequenceClassification
config = BertConfig.from_pretrained('bert-base-chinese', num_labels=2)  #num_labels 設定類別數
model = BertForSequenceClassification.from_pretrained("bert-base-chinese",config=config)

#print(model)

"""## 該來訓練模型囉"""

import logging
import datasets
from datasets import load_dataset, load_metric
from torch.utils.data import DataLoader
from tqdm.auto import tqdm, trange
import math

import transformers
from accelerate import Accelerator
from transformers import (
    AdamW,
    AutoConfig,
    default_data_collator,
    get_scheduler
)

train_batch_size = 12      # 設定 training batch size 
eval_batch_size = 12      # 設定 eval batch size
num_train_epochs = 1      # 設定 epoch

data_collator = default_data_collator
train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=train_batch_size)
eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=eval_batch_size)

learning_rate=3e-5          # 設定 learning_rate
gradient_accumulation_steps = 1   # 設定 幾步後進行反向傳播

no_decay = ["bias", "LayerNorm.weight"]
optimizer_grouped_parameters = [
    {
        "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
        "weight_decay": 0.0,
    },                                
    {
        "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
        "weight_decay": 0.0,
    },
]
optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)

# Scheduler and math around the number of training steps.
num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
max_train_steps = num_train_epochs * num_update_steps_per_epoch
print('max_train_steps', max_train_steps)

# scheduler
lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=max_train_steps,
)

# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
accelerator = Accelerator()

# Prepare everything with our `accelerator`.
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)

eval_dataloader
metric = load_metric("accuracy")

"""真正開始訓練"""

logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
logger.info(accelerator.state)
output_dir = '/content'  # your folder
 

total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps

logger.info("***** Running training *****")
logger.info(f"  Num examples = {len(train_dataset)}")
logger.info(f"  Num Epochs = {num_train_epochs}")
logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
logger.info(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
logger.info(f"  Total optimization steps = {max_train_steps}")


completed_steps = 0
best_epoch = {"epoch": 0, "acc": 0 }

for epoch in trange(num_train_epochs, desc="Epoch"):#trange是print進度條的方式
  model.train()
  for step, batch in enumerate(tqdm(train_dataloader, desc="Iteration")):
    outputs = model(**batch)
    #loss = outputs
    loss = outputs.loss
    #loss = loss / gradient_accumulation_steps
    accelerator.backward(loss)
    #if step % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1: 把if刪掉了
    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()
    completed_steps += 1

    if step % 50 == 0:
      print({'epoch': epoch, 'step': step, 'loss': loss.item()})

    if completed_steps >= max_train_steps:
      break
      
  logger.info("***** Running eval *****")
  model.eval()
  for step, batch in enumerate(tqdm(eval_dataloader, desc="eval Iteration")):
    outputs = model(**batch)
    predictions = outputs.logits.argmax(dim=-1)
    metric.add_batch(
        predictions=accelerator.gather(predictions),
        references=accelerator.gather(batch["labels"]),
    )

  eval_metric = metric.compute()
  logger.info(f"epoch {epoch}: {eval_metric}")
  if eval_metric['accuracy'] > best_epoch['acc']:
    best_epoch.update({"epoch": epoch, "acc": eval_metric['accuracy']})

  if output_dir is not None:
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir + '/' + 'epoch_' + str(epoch), save_function=accelerator.save)

"""## 最好的某次訓練成果"""

print(best_epoch)

"""# 訓練成果驗證"""

from transformers import BertTokenizerFast, BertConfig, BertForSequenceClassification, default_data_collator
from torch.utils.data import DataLoader
from accelerate import Accelerator
from tqdm.auto import tqdm

#cd drive

#ls

config = BertConfig.from_pretrained("./content/epoch_"+best_epoch['epoch']+"/config.json") 
model = BertForSequenceClassification.from_pretrained("./content/epoch_"+best_epoch['epoch']+"/pytorch_model.bin", config = config).to(device)

from transformers import BertTokenizerFast, BertConfig, BertForSequenceClassification

def mrpc_model(model, sen1, sen2):
  input_encodings = tokenizer([sen1], [sen2], padding='max_length', truncation=True)
  input_dataset = Dataset(input_encodings)
  #print(input_encodings)
  #print(input_dataset[0])
  data_collator = default_data_collator
  input_dataloader = DataLoader(input_dataset, collate_fn=data_collator, batch_size=1)  

  accelerator = Accelerator()
  model, input_dataloader = accelerator.prepare(model, input_dataloader)

  for batch in input_dataloader:
    outputs = model(**batch)
    predicted = outputs.logits.argmax(dim=-1)
  return predicted

"""### 可以拿來玩的地方"""

sen1="lisa goes to school everyday"
sen2="lisa everyday goes to school"
#sen1="lisa is a singer"
#sen2="lisa is not a singer"

predict = mrpc_model(model, sen1, sen2)
print("sentence= : ", sen1)
print("sentence= : ", sen2)

print("predict_label : ", predict.item())
if predict.item():
  print("有關聯")
else:
  print("沒關聯")

cnt=0
errorcnt=0
test_sen1,test_sen2,testlabel
for i in range(len(testlabel)):
    cnt+=1
    sen1=test_sen1[i]
    sen2=test_sen2[i]
    predict=mrpc_model(model,sen1,sen2)
    if predict.item()!=test_data['label'][i]:
        errorcnt+=1
print("cnt = ",cnt)
print("errorcnt = ",errorcnt)

"""結果存進google drive

from google.colab import drive
drive.mount('/content/gdrive')
"""

#cd gdrive/MyDrive/Colab Notebooks

torch.save(model,"./test_model2.bin")